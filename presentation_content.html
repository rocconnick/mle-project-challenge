<!DOCTYPE html>
<html>
  <head>
    <title>phData MLE Project Solution</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Raleway:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Raleway'; }
      h1, h2, h3 {
        font-family: 'Raleway';
        font-weight: bold;
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

# Machine Learning Engineer Candidate Project

---

## About Me

* At work
  * Particle-physics PhD: computer vision, 100+ TB datasets, "grid" processing
  * Data science at 3M: ML/DL/NLP algorithm development on millions of health records

* Outside of work
  * Ride bikes
  * Travel and hike
  * Grow, cook, and eat food

---

## Project Solution Design

* Factored into two services so API to remains active during model update
    * Internal model service – versions and serves latest serialized model
    * External API – serves up predictions via POST request

* Implemented as Docker containers to allow scaling via container orchestration

---

## Solution Shortcomings

* Load balancing and proxy - Kubernetes or cloud container service can handle that

* Web interface for users to manually submit requests

* External logging - any number of loging services could be integrated

* Lots of sugar – not a lot of logic here, played fast & loose
      * Integration/unit tests
      * Schema validation

---

## Implementation

* Both services are Flask applications in Docker containers

* Using Miniconda as a base Docker image
    * Advantage: environment YAML files are easy to read
    * Disadvantage: Conda environment interface is janky, poorly documented

```docker
FROM continuumio/miniconda3:latest

ADD conda_environment.yml .
RUN conda update -n base -c defaults conda && \
    conda env create -f ./conda_environment.yml

ADD predict.py .

# Default port for Flask applications
EXPOSE 5000

CMD ["conda", "run", "-n", "loanApp", "python", "predict.py"]
```

---

## Predict API Details

* Predict API just makes predictions, does not handle model files

* Model version checked on every request, updated if necessary

* Output includes model version, predicted probability, and class label

```python
@app.route('/predict', methods=['POST'])
def predict() -> str:
    """
    Return model predictions as JSON string.
    """
    global model, model_hash
    # TODO(drocco): implement input schema validation here
    data = flask.request.get_json()
    input_df = pd.DataFrame.from_dict(data)

    model, model_hash = update_model(model, model_hash)

    prediction = model.predict(input_df)
    prediction_proba = model.predict_proba(input_df)
    output = {"prediction": int(prediction[0]),
              "predict_proba": list(prediction_proba[0]),
              "model_version": model_hash}

    return flask.jsonify(output)
```

---


## Model Server Details

* Loads serialized model from object store
    * Model just stored on a Docker volume for this prototype
    * Should really be something like Amazon S3 or other scaleable object store

* Custom Flask application for desired model-versioning scheme (better open-source options?)
    * Model version is tracked via SHA hash
    * File "URI" and hash are configured in `docker-compose.yml`
    * Hash is validated internally

```yaml
model:
    build:
      context: ./services/model
      dockerfile: Dockerfile
    volumes:
      - ./models:/etc/models:ro
    environment:
      MODEL_DIR: /etc/models
      MODEL_FILE_NAME: latePaymentsModel.pkl
      MODEL_SHA: 0d1ce1a10c5df544b7b660f29a0ea02180dfd7f7


```
* Model server can be restarted with a new model while predict API remains running

---

## Model Improvement

* Added some exploratory plots and figures, model inputs looked decent enough for ML
  * descriptive stats: some extreme outliers, e.g. income variable (decision trees are robust though)
  * histograms: inlier values reasonably distributed; categoricals have just a few values
  * null counts: no features had extreme null rates

* Extreme class imbalance - late-payment probability never exceeds 50% for test set
    * Accuracy/precision/recall all don't measure anything meaningful
    * ROC-AUC is better, appropriate interpretation: "probability two randomly sampled predictions are ranked correctly"

* Optimized Random-Forest model with cross-validated grid search; tried Gradient Boosted Model

```python
param_grid = {"classifier__n_estimators": [10, 20, 30],
              "classifier__max_depth": [3, 10, 20],
              "classifier__class_weight": ["balanced", None],
              "preprocess__num__scaler": [RobustScaler(), None]
              }
```

---
## Model Improvement Results



| Model                   | Train AUC | Test AUC |                              |
|:------------------------|:---------:|:--------:|:------------------------------------|
| Original Random Forest  |    1.0    | 0.62     | Poor generalization                 |
| Optimized Random Forest |    0.85   | 0.74     | Better, still overfit               |
| Gradient Boosted Model  |    0.75   | 0.75     | Good performance &  generalization  |

<br />
<br />
.right[
![ROC Curve](img/roc_auc.png "ROC Curve")
]

---
class: middle, center
## Demo


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create({ ratio: "16:9" });
    </script>
  </body>
</html>